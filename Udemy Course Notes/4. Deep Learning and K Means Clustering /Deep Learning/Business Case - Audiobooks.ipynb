{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_csv_data = np.loadtxt('/Users/jackcook/Documents/Course notes/Data Sources/business-case-books.csv',\n",
    "                         delimiter=',')\n",
    "\n",
    "# Exclude ID column & target\n",
    "unscaled_inputs_raw = raw_csv_data[:,1:-1]\n",
    "\n",
    "targets_raw = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-shuffle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ensures that we aren't picking any rows more favourably over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arange returns evenly spaced values in a given interval\n",
    "shuffled_indices = np.arange(unscaled_inputs_raw.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = unscaled_inputs_raw[shuffled_indices]\n",
    "shuffled_targets = targets_raw[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4474, 10) (4474,)\n",
      "2237.0\n"
     ]
    }
   ],
   "source": [
    "num_one_targets = shuffled_targets.sum()\n",
    "\n",
    "# Count zeroes and record positions\n",
    "zero_targets_count = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(shuffled_inputs.shape[0]):\n",
    "    if shuffled_targets[i] == 0:\n",
    "        zero_targets_count += 1\n",
    "        # Note indices to remove\n",
    "        if zero_targets_count > num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "            \n",
    "def delete_data(array):\n",
    "    return np.delete(array, indices_to_remove, axis=0)\n",
    "            \n",
    "unscaled_inputs_balanced = delete_data(shuffled_inputs)\n",
    "targets_balanced = delete_data(shuffled_targets)\n",
    "\n",
    "print(unscaled_inputs_balanced.shape, targets_balanced.shape)\n",
    "print(np.sum(targets_balanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arange returns evenly spaced values in a given interval\n",
    "reshuffled_indices = np.arange(unscaled_inputs_balanced.shape[0])\n",
    "np.random.shuffle(reshuffled_indices)\n",
    "\n",
    "reshuffled_bal_inputs = unscaled_inputs_balanced[reshuffled_indices]\n",
    "reshuffled_bal_targets = targets_balanced[reshuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4474, 10) 2237.0\n"
     ]
    }
   ],
   "source": [
    "scaled_inputs = preprocessing.scale(reshuffled_bal_inputs)\n",
    "\n",
    "print(scaled_inputs.shape, np.sum(reshuffled_bal_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples count: 4474\n",
      "Train count: 3579\n",
      "Val count: 447\n",
      "Test count: 448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "samples_count = scaled_inputs.shape[0]\n",
    "\n",
    "# 80 : 10 : 10 split\n",
    "train_samples_count = int(samples_count*0.8)\n",
    "val_samples_count = int(samples_count*0.1)\n",
    "test_samples_count = samples_count - train_samples_count - val_samples_count\n",
    "\n",
    "train_inputs = scaled_inputs[:train_samples_count]\n",
    "val_inputs = scaled_inputs[train_samples_count : - test_samples_count]\n",
    "test_inputs = scaled_inputs[- test_samples_count:]\n",
    "\n",
    "print('Samples count: {}\\nTrain count: {}\\nVal count: {}\\nTest count: {}\\n'.format(\n",
    "    samples_count,\n",
    "    train_inputs.shape[0],\n",
    "    val_inputs.shape[0],\n",
    "    test_inputs.shape[0]))\n",
    "\n",
    "# Then split the targets in the same way\n",
    "train_targets = reshuffled_bal_targets[:train_samples_count]\n",
    "val_targets = reshuffled_bal_targets[train_samples_count : - test_samples_count]\n",
    "test_targets = reshuffled_bal_targets[- test_samples_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the datasets are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples % 1s: 50.0\n",
      "Train % 1s: 50.0\n",
      "Val % 1s: 49.0\n",
      "Test % 1s: 52.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Samples % 1s: {}\\nTrain % 1s: {}\\nVal % 1s: {}\\nTest % 1s: {}\\n'.format(\n",
    "    round(100 * np.sum(targets_balanced)/samples_count),\n",
    "    round(100 * np.sum(train_targets)/train_samples_count),\n",
    "    round(100 * np.sum(val_targets)/val_samples_count),\n",
    "    round(100 * np.sum(test_targets)/test_samples_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save as .csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just an exercise as this helps up practise loading numpy npz data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_val', inputs=val_inputs, targets=val_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_train = np.load('Audiobooks_data_train.npz')\n",
    "\n",
    "train_inputs = npz_train['inputs'].astype(np.float)\n",
    "train_targets = npz_train['targets'].astype(np.int)\n",
    "\n",
    "npz_val = np.load('Audiobooks_data_val.npz')\n",
    "\n",
    "val_inputs = npz_val['inputs'].astype(np.float)\n",
    "val_targets = npz_val['targets'].astype(np.int)\n",
    "\n",
    "npz_test = np.load('Audiobooks_data_test.npz')\n",
    "\n",
    "test_inputs = npz_test['inputs'].astype(np.float)\n",
    "test_targets = npz_test['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3579 samples, validate on 447 samples\n",
      "Epoch 1/100\n",
      "3579/3579 - 1s - loss: 0.4351 - accuracy: 0.7720 - val_loss: 0.3724 - val_accuracy: 0.8076\n",
      "Epoch 2/100\n",
      "3579/3579 - 0s - loss: 0.3805 - accuracy: 0.7952 - val_loss: 0.3873 - val_accuracy: 0.8076\n",
      "Epoch 3/100\n",
      "3579/3579 - 0s - loss: 0.3740 - accuracy: 0.7974 - val_loss: 0.3679 - val_accuracy: 0.7852\n",
      "Epoch 4/100\n",
      "3579/3579 - 0s - loss: 0.3660 - accuracy: 0.7966 - val_loss: 0.3889 - val_accuracy: 0.7629\n",
      "Epoch 5/100\n",
      "3579/3579 - 0s - loss: 0.3693 - accuracy: 0.7977 - val_loss: 0.3513 - val_accuracy: 0.8345\n",
      "Epoch 6/100\n",
      "3579/3579 - 0s - loss: 0.3667 - accuracy: 0.7958 - val_loss: 0.3575 - val_accuracy: 0.8098\n",
      "Epoch 7/100\n",
      "3579/3579 - 1s - loss: 0.3553 - accuracy: 0.8061 - val_loss: 0.3524 - val_accuracy: 0.8210\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142ef7a58>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2 # Purchase or no purchase\n",
    "hidden_layer_size = 150\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # We don't need to flatten our inputs this time as it's already the right shape\n",
    "    \n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy', # Applies one-hot encoding to targets\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "batch_size = 10\n",
    "max_epochs = 100\n",
    "\n",
    "#Â By running for all 100 epochs we risk overfitting\n",
    "# so, we should introduce early stopping\n",
    "# Callbacks are utilities called at certain points during training\n",
    "\n",
    "# Stops training when validation loss doesn't improve\n",
    "# Patience allows n number of epochs where val loss increases before stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "model.fit(train_inputs,\n",
    "          train_targets, \n",
    "          batch_size=batch_size,\n",
    "          epochs=max_epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(val_inputs, val_targets),\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 0s 182us/sample - loss: 0.3690 - accuracy: 0.8058\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.37\n",
      "Test accuracy:  0.81%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0: .2f}\\nTest accuracy: {1: .2f}%'.format(test_loss,test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
