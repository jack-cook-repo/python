{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Prepare data\n",
    "2. Outline model & activation functions\n",
    "3. Set appropriate optimisers and loss function\n",
    "4. Learn\n",
    "5. Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jackcook/.pyenv/versions/3.7.1/lib/python3.7/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset plus information\n",
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", as_supervised=True, with_info=True)\n",
    "\n",
    "# Train/test split\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# Ensure this is an integer\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "# Ensure this is an integer\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then scale inputs between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    # Take image\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # Value between 0 and 255, divide by 255 to get 0-1 range\n",
    "    image /= 255. # Dot forces float64 output\n",
    "    return image, label\n",
    "\n",
    "\n",
    "# Remember we are using our training dataset for train & validation\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "\n",
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffling means keeping the same information, but in a different order.\n",
    "\n",
    "This helps prevent issues when batching data - ie. first 1000 examples are all target label 0, second 100 examples are 50:50 split of 0 & 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If buffer size = 1, no shuffling happens\n",
    "- If buffer size >= number of samples, all samples shuffled uniformly\n",
    "- Inbetween optimises for computing power (larger size = more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Take and skip include & omit a given number of samples respectively\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change batch size to determine what type of gradient descent (GD) we perform:\n",
    "\n",
    "- Size of 1: Stochastic GD\n",
    "- Size of whole dataset: GD\n",
    "- Inbetween: Batch GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "train_data = train_data.batch(BATCH_SIZE)\n",
    "\n",
    "# We don't need to batch validation data, as we don't BACK propogate the weights using it\n",
    "# As a result it's much less computationally intensive\n",
    "# We just provide the whole sample as a batch to calculate avg. loss each epoch\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "\n",
    "# The same goes for test data\n",
    "test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the validation inputs & targets by unpacking the tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter() creates an objected which can be looped through\n",
    "# next() loads the next element, in this case we just have a single batch\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 784 inputs (28 x 28 pixels, each with a value of 0 to 1), 10 output digits, and we will be using 2 hidden layers with 50 nodes in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 200 # We can use different widths for each layer if we like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([ # Stacking layers\n",
    "    # Flatten turns a tensor into a vector\n",
    "    # In our case, it takes a 28x28x1 tensor and produces a vector of 784\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    \n",
    "    # Dense takes inputs and calculates dot product of inputs x weights\n",
    "    # Then adds bias, and applies activation function\n",
    "    \n",
    "    # Hidden layers 1-5\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    \n",
    "    # Output layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective (loss) function & optimisation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ADAptive Moment (ADAM) estimation as optimisation algorithm. ADAM uses:\n",
    "\n",
    "- RMSprop (Root Mean Square Propogation) scheduler (updates eta during training)\n",
    "- Momentum (adding part of previous update to current update) to reflect \"speed\" of descent)\n",
    "\n",
    "We use cross-entropy to see how well our model performs vs. actual digits:\n",
    "\n",
    "- Binary cross-entropy requires binary encoding (not our case)\n",
    "- Categoryical cross-entropy assumes you've one-hot encoded targets\n",
    "- Sparse categorical cross-entropy applies one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 7s - loss: 0.2693 - accuracy: 0.9183 - val_loss: 0.1243 - val_accuracy: 0.9627\n",
      "Epoch 2/5\n",
      "540/540 - 6s - loss: 0.1062 - accuracy: 0.9676 - val_loss: 0.0875 - val_accuracy: 0.9737\n",
      "Epoch 3/5\n",
      "540/540 - 5s - loss: 0.0755 - accuracy: 0.9766 - val_loss: 0.0779 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "540/540 - 6s - loss: 0.0553 - accuracy: 0.9818 - val_loss: 0.0607 - val_accuracy: 0.9812\n",
      "Epoch 5/5\n",
      "540/540 - 6s - loss: 0.0471 - accuracy: 0.9844 - val_loss: 0.0665 - val_accuracy: 0.9787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1445bd1d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "model.fit(train_data, \n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=(validation_inputs,validation_targets),\n",
    "          validation_steps=1, # We only provide 1 batch of validation data\n",
    "          verbose=2) # Only print loss for each epoch\n",
    "\n",
    "# With a batch size of 100, each epoch has 540 updates (54,000 training samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown - 1s 623ms/step - loss: 0.0900 - accuracy: 0.9745"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  0.09. Test accuracy:  97.45%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0: .2f}. Test accuracy: {1: .2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save images into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = mnist_test.__iter__()\n",
    "images=[]\n",
    "for mnist_example in iterator: \n",
    "    images.append(mnist_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_example = images[1] \n",
    "img, label = mnist_example[0], mnist_example[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x145219438>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img.numpy()[:, :, 0].astype(np.float32), cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual class: 0\n",
      "predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "img = tf.expand_dims(img, axis=0)      \n",
    "img = scale(img, label)\n",
    "pred = model.predict_classes(img)[0]\n",
    "print('actual class:', label.numpy())\n",
    "print('predicted class:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
